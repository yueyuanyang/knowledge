## 文本分类,数据挖掘和机器学习

机器学习的有概率分类器(probabilistic) ,贝叶斯推理网络(bayesian inference networks) , 决策树分类器(decision tree) ,决策规则分类器(decision rule) ,基于回归的线性最小二乘llsf(regression based on linearleast squares fit ) , 符号规则归纳法( symbolic rule induction) ,中心向量法(rocchio) ,神经网络法(neural networks) ,k 近邻法(knn) ,支持向量机法(svm) ,投票委员会(majority voting ) , 遗传算法( genetic algorithm) , 最大熵算法(maximum entropy) , ecoc(error correcting output coding) ,等等。这些分类算法成为目前文本分类的主流,在不同的领域里取得了较好的效果。

究竟哪一种算法性能好些至今没有一个定论。实验表明knn ,svm 和贝叶斯分类器的性能比较好。

**(一)文本分类问题的定义**

一个文本（以下基本不区分“文本”和“文档”两个词的含义）**分类问题就是将一篇文档归入预先定义的几个类别中的一个或几个，而文本的自动分类则是使用计算机程序来实现这样的分类。**

注意这个定义当中着重强调的两个事实。

第一，用于分类所需要的类别体系是预先确定的。例如新浪新闻的分类体系，Yahoo!网页导航的分类层次。这种分类层次一旦确定，在相当长的时间内都是不可变的，或者即使要变更，也要付出相当大的代价（基本不亚于推倒并重建一个分类系统）。

第二，一篇文档并没有严格规定只能被分配给一个类别。这与分类这个问题的主观性有关，例如找10个人判断一篇文章所陈述的主题究竟属于金融，银行还是财政政策领域，10个人可能会给出10个不同的答案，因此一篇文章很可能被分配到多个类别当中，只不过分给某些类别让人信服，而有些让人感觉模棱两可罢了（置信度不一样）。

当然，目前真正大量使用文本分类技术的，仍是依据文章主题的分类，而据此构建最多的系统，当属搜索引擎。内里的原因当然不言自明，我只是想给大家提个醒，文本分类还不完全等同于网页分类。网页所包含的信息远比含于其中的文字（文本）信息多得多，对一个网页的分类，除了考虑文本内容的分类以外，链入链出的链接信息，页面文件本身的元数据，甚至是包含此网页的网站结构和主题，都能给分类提供莫大的帮助（比如新浪体育专栏里的网页毫无疑问都是关于体育的），因此说文本分类实际上是网页分类的一个子集也毫不为过。当然，纯粹的文本分类系统与网页分类也不是一点区别都没有。文本分类有个重要前提：即只能根据文章的文字内容进行分类，而不应借助诸如文件的编码格式，文章作者，发布日期等信息。而这些信息对网页来说常常是可用的，有时起到的作用还很巨大！因此纯粹的文本分类系统要想达到相当的分类效果，必须在本身的理论基础和技术含量上下功夫。

除了搜索引擎，诸如数字图书馆，档案管理等等要和海量文字信息打交道的系统，都用得上文本分类。

**(二)文本分类的方法**

文本分类问题与其它分类问题没有本质上的区别，其方法可以归结为根据待分类数据的某些特征来进行匹配，当然完全的匹配是不太可能的，因此必须（根据某种评价标准）选择最优的匹配结果，从而完成分类。

　　因此核心的问题便转化为用哪些特征表示一个文本才能保证有效和快速的分类（注意这两方面的需求往往是互相矛盾的）。因此自有文本分类系统的那天起，就一直是对特征的不同选择主导着方法派别的不同。

　　**最早的词匹配法仅仅根据文档中是否出现了与类名相同的词（顶多再加入同义词的处理）来判断文档是否属于某个类别。**很显然，这种过于简单的方法无法带来良好的分类效果。

　　**后来兴起过一段时间的知识工程的方法则借助于专业人员的帮助，为每个类别定义大量的推理规则，**如果一篇文档能满足这些推理规则，则可以判定属于该类别。这里与特定规则的匹配程度成为了文本的特征。由于在系统中加入了人为判断的因素，准确度比词匹配法大为提高。但这种方法的缺点仍然明显，例如分类的质量严重依赖于这些规则的好坏，也就是依赖于制定规则的“人”的好坏；再比如制定规则的人都是专家级别，人力成本大幅上升常常令人难以承受；而知识工程最致命的弱点是完全不具备可推广性，一个针对金融领域构建的分类系统，如果要扩充到医疗或社会保险等相关领域，则除了完全推倒重来以外没有其他办法，常常造成巨大的知识和资金浪费。

　　后来人们意识到，究竟依据什么特征来判断文本应当隶属的类别这个问题，就连人类自己都不太回答得清楚，有太多所谓“只可意会，不能言传”的东西在里面。**人类的判断大多依据经验以及直觉，因此自然而然的会有人想到何让机器像人类一样自己来通过对大量同类文档的观察来自己总结经验，作为今后分类的依据。**

　　**这便是统计学习方法的基本思想（也有人把这一大类方法称为机器学习**，两种叫法只是涵盖范围大小有些区别，均无不妥）。

　　统计学习方法需要一批由人工进行了准确分类的文档作为学习的材料（称为训练集，注意由人分类一批文档比从这些文档中总结出准确的规则成本要低得多），计算机从这些文档重挖掘出一些能够有效分类的规则，这个过程被形象的称为训练，而总结出的规则集合常常被称为分类器。训练完成之后，需要对计算机从来没有见过的文档进行分类时，便使用这些分类器来进行。

　　**现如今，统计学习方法已经成为了文本分类领域绝对的主流**。主要的原因在于其中的很多技术拥有坚实的理论基础（相比之下，知识工程方法中专家的主观因素居多），存在明确的评价标准，以及实际表现良好。
  
**(三)统计学习方法**

前文说到使用统计学习方法进行文本分类就是让计算机自己来观察由人提供的训练文档集，自己总结出用于判别文档类别的规则和依据。理想的结果当然是让计算机在理解文章内容的基础上进行这样的分类，然而遗憾的是，我们所说的“理解”往往指的是文章的语义甚至是语用信息，这一类信息极其复杂，抽象，而且存在上下文相关性，对这类信息如何在计算机中表示都是尚未解决的问题（是一个“知识表示”的问题），更不要说让计算机来理解。

**利用计算机来解决问题的标准思路应该是：为这种问题寻找一种计算机可以理解的表示方法，或曰建立一个模型（一个文档表示模型）；然后基于这个模型，选择各方面满足要求的算法来解决。**

既然文本的语义和语用信息很难转换成计算机能够理解的表示形式，接下来顺理成章的，人们开始用文章中所包含的较低级别的词汇信息来表示文档。

统计学习方法进行文本分类（以下就简称为“统计学习方法”，虽然这个方法也可以应用到除文本分类以外的多个领域）的一个重要前提由此产生，那就是认为：**文档的内容与其中所包含的词有着必然的联系，同一类文档之间总存在多个共同的词，而不同类的文档所包含的词之间差异很大[1]。**

进一步的，不光是包含哪些词很重要，这些词出现的次数对分类也很重要。

**这一前提使得向量模型（俗称的VSM，向量空间模型）成了适合文本分类问题的文档表示模型**。在这种模型中，一篇文章被看作特征项集合来看，利用加权特征项构成向量进行文本表示，利用词频信息对文本特征进行加权。它实现起来比较简单，并且分类准确度也高，能够满足一般应用的要求。[5]

而实际上，文本是一种信息载体，其所携带的信息由几部分组成：如组成元素本身的信息（词的信息）、组成元素之间顺序关系带来的信息以及上下文信息（更严格的说，还包括阅读者本身的背景和理解）[12]。

**而VSM这种文档表示模型，基本上完全忽略了除词的信息以外所有的部分，这使得它能表达的信息量存在上限[12]，**也直接导致了基于这种模型构建的文本分类系统（虽然这是目前绝对主流的做法），几乎永远也不可能达到人类的分类能力。后面我们也会谈到，相比于所谓的分类算法，对特征的选择，也就是使用哪些特征来代表一篇文档，往往更能影响分类的效果。

**对于扩充文档表示模型所包含的信息量，人们也做过有益的尝试，例如被称为LSI（Latent Semantic Index潜在语义索引）的方法**，就被实验证明保留了一定的语义信息（之所以说被实验证明了，是因为人们还无法在形式上严格地证明它确实保留了语义信息，而且这种语义信息并非以人可以理解的方式被保留下来），此为后话。

前文说到（就不能不用这种老旧的说法？换换新的，比如Previously on "Prison Break"，噢，不对，是Previously on Text Categorizaiton……）统计学习方法其实就是一个两阶段的解决方案，**（1）训练阶段，由计算机来总结分类的规则；（2）分类阶段，给计算机一些它从来没见过的文档，让它分类。**

**(四)训练1**

**训练，就是training，简单的说就是让计算机从给定的一堆文档中自己学习分类的规则。**

开始训练之前，再多说几句关于VSM这种文档表示模型的话。

举个例子，假设说把我正在写的“文本分类入门”系列文章的第二篇抽出来当作一个需要分类的文本，则可以用如下的向量来表示这个文本，以便于计算机理解和处理。

w2=（文本，5，统计学习，4，模型，0，……）

这个向量表示在w2所代表的文本中，“文本”这个词出现了5次（这个信息就叫做词频），“统计学习”这个词出现了4次，而“模型”这个词出现了0次，依此类推，后面的词没有列出。

而系列的第三篇文章可以表示为

w3=（文本，9，统计学习，4，模型，10，……）

其含义同上。如果还有更多的文档需要表示，我们都可以使用这种方式。

只通过观察w2和w3我们就可以看出实际上有更方便的表示文本向量的方法，那就是把所有文档都要用到的词从向量中抽离出来，形成共用的数据结构（也可以仍是向量的形式），**这个数据结构就叫做词典，或者特征项集合。**

例如我们的问题就可以抽离出一个词典向量

D=（文本，统计学习，模型，……）

所有的文档向量均可在参考这个词典向量的基础上简化成诸如

w2=（5，4，0，……）

w3=（9，4，10，……）

的形式，其含义没有改变。

5，4，10这些数字分别叫做各个词在某个文档中的权重，实际上单单使用词频作为权重并不多见，也不十分有用，更常见的做法是使用地球人都知道的**TF/IDF值作为权重**。（关于TF/IDF的详细解释，Google的吴军研究员写了非常通俗易懂的文章，发布于Google黑板报，**TF/IDF作为一个词对所属文档主题的贡献程度来说，是非常重要的度量标准，也是将文档转化为向量表示过程中的重要一环**。

在这个转化过程中隐含了一个很严重的问题。注意看看词典向量D，你觉得它会有多大？或者说，你觉得它会包含多少个词？

假设我们的系统仅仅处理汉语文本，如果不做任何处理，这个词典向量会包含汉语中所有的词汇，我手头有一本商务印书馆出版的《现代汉语词典》第5版（2005年5月出版），其中收录了65，000个词，D大致也应该有这么大，也就是说，D是一个65，000维的向量，而所有的文本向量w2,w3,wn也全都是65，000维的！（这是文本分类这一问题本身的一个特性，称为“高维性”）想一想，大部分文章仅仅千余字，包含的词至多几百，为了表示这样一个文本，却要使用65，000维的向量，这是对存储资源和计算能力多大的浪费呀！（这又是文本分类问题的另一个特性，称为“向量稀疏性”，后面会专门有一章讨论这些特性，并指出解决的方法，至少是努力的方向）

这么多的词汇当中，诸如“体育”，“经济”，“金融”，“处理器”等等，都是极其能够代表文章主题的，但另外很多词，像“我们”，“在”，“事情”，“里面”等等，在任何主题的文章中都很常见，根本无法指望通过这些词来对文本类别的归属作个判断。**这一事实首先引发了对文本进行被称为“去停止词”的预处理步骤（对英文来说还有词根还原**，但这些与训练阶段无关，不赘述，会在以后讲述中英文文本分类方法区别的章节中讨论），与此同时，我们也从词典向量D中把这些词去掉。

**但经过停止词处理后剩下的词汇仍然太多，使用了太多的特征来表示文本，就是常说的特征集过大，不仅耗费计算资源，也因为会引起“过拟合问题”而影响分类效果[22]。**

**这个问题是训练阶段要解决的第一个问题，即如何选取那些最具代表性的词汇（更严格的说法应该是，那些最具代表性的特征，为了便于理解，可以把特征暂时当成词汇来想象）。对这个问题的解决，有人叫它特征提取，也有人叫它降维。**

**特征提取**实际上有两大类方法。一类称为**特征选择（Term Selection）**，指的是从原有的特征（那许多有用无用混在一起的词汇）中提取出少量的，具有代表性的特征，但特征的类型没有变化（原来是一堆词，特征提取后仍是一堆词，数量大大减少了而已）。另一类称为**特征抽取（Term Extraction）**的方法则有所不同，它从原有的特征中重构出新的特征（原来是一堆词，重构后变成了别的，**例如LSI将其转为矩阵，文档生成模型将其转化为某个概率分布的一些参数），新的特征具有更强的代表性，并耗费更少的计算资源。（特征提取的各种算法会有专门章节讨论）**

**训练阶段，计算机根据训练集中的文档，使用特征提取找出最具代表性的词典向量（仍然是不太严格的说法），然后参照这个词典向量把这些训练集文档转化为向量表示，之后的所有运算便都使用这些向量进行，不再理会原始的文本形式的文档了。**
  
 **(五)训练2** 
 
 将样本数据成功转化为向量表示之后，计算机才算开始真正意义上的“学习”过程。

再重复一次，所谓样本，也叫训练数据，是由人工进行分类处理过的文档集合，计算机认为这些数据的分类是绝对正确的，可以信赖的（但某些方法也有针对训练数据可能有错误而应对的措施）。**接下来的一步便是由计算机来观察这些训练数据的特点，来猜测一个可能的分类规则（这个分类规则也可以叫做分类器，在机器学习的理论著作中也叫做一个“假设”**，因为毕竟是对真实分类规则的一个猜测），一旦这个分类满足一些条件，我们就认为这个分类规则大致正确并且足够好了，便成为训练阶段的最终产品——分类器！再遇到新的，计算机没有见过的文档时，便使用这个分类器来判断新文档的类别。

举一个现实中的例子，人们评价一辆车是否是“好车”的时候，可以看作一个分类问题。我们也可以把一辆车的所有特征提取出来转化为向量形式。在这个问题中词典向量可以为：

 D=（价格，最高时速，外观得分，性价比，稀有程度）

则一辆保时捷的向量表示就可以写成

 vp=（200万，320，9.5，3，9）

而一辆丰田花冠则可以写成

 vt=（15万，220，6.0，8，3）

找不同的人来评价哪辆车算好车，很可能会得出不同的结论。务实的人认为性价比才是评判的指标，他会认为丰田花冠是好车而保时捷不是；喜欢奢华的有钱人可能以稀有程度来评判，得出相反的结论；喜欢综合考量的人很可能把各项指标都加权考虑之后才下结论。

可见，**对同一个分类问题，用同样的表示形式（同样的文档模型），但因为关注数据不同方面的特性而可能得到不同的结论**。这种对文档数据不同方面侧重的不同导致了原理和实现方式都不尽相同的多种方法，每种方法也都对文本分类这个问题本身作了一些有利于自身的假设和简化，这些假设又接下来影响着依据这些方法而得到的分类器最终的表现，可谓环环相连。

比较常见，家喻户晓，常年被评为国家免检产品的分类算法有一大堆，**什么决策树，Rocchio，朴素贝叶斯，神经网络，支持向量机，线性最小平方拟合，k-NN，遗传算法，最大熵，Generalized Instance Set等等等等。在这里只挑几个最具代表性的算法侃一侃**。

**Rocchio算法**

Rocchio算法应该算是人们思考文本分类问题时最先能想到，也最符合直觉的解决方法。基本的思路是把一个类别里的样本文档各项取个平均值（例如把所有“体育”类文档中词汇“篮球”出现的次数取个平均值，再把“裁判”取个平均值，依次做下去），可以得到一个新的向量，形象的称之为“质心”，质心就成了这个类别最具代表性的向量表示。再有新文档需要判断的时候，比较新文档和质心有多么相像（八股点说，判断他们之间的距离）就可以确定新文档属不属于这个类。稍微改进一点的Rocchio算法不尽考虑属于这个类别的文档（称为正样本），也考虑不属于这个类别的文档数据（称为负样本），计算出来的质心尽量靠近正样本同时尽量远离负样本。Rocchio算法做了两个很致命的假设，使得它的性能出奇的差。一是它认为一个类别的文档仅仅聚集在一个质心的周围，实际情况往往不是如此（这样的数据称为线性不可分的）；二是它假设训练数据是绝对正确的，因为它没有任何定量衡量样本是否含有噪声的机制，因而也就对错误数据毫无抵抗力。

　　不过Rocchio产生的分类器很直观，很容易被人类理解，算法也简单，还是有一定的利用价值的（做汉奸状），常常被用来做科研中比较不同算法优劣的基线系统（Base Line）。

**朴素贝叶斯算法（Naive Bayes）**

贝叶斯算法关注的是文档属于某类别概率。文档属于某个类别的概率等于文档中每个词属于该类别的概率的综合表达式。而每个词属于该类别的概率又在一定程度上可以用这个词在该类别训练文档中出现的次数（词频信息）来粗略估计，因而使得整个计算过程成为可行的。使用朴素贝叶斯算法时，在训练阶段的主要任务就是估计这些值。

朴素贝叶斯算法的公式只有一个

**文本分类入门(五)训练Part 2**

其中P(d| Ci)=P(w1|Ci) P(w2|Ci) …P(wi|Ci) P(w1|Ci) …P(wm|Ci) （式1）

P(wi|Ci)就代表词汇wi属于类别Ci的概率。

这其中就蕴含着朴素贝叶斯算法最大的两个缺陷。

首先，P(d| Ci)之所以能展开成（式1）的连乘积形式，就是假设一篇文章中的各个词之间是彼此独立的，其中一个词的出现丝毫不受另一个词的影响（回忆一下概率论中变量彼此独立的概念就可以知道），但这显然不对，即使不是语言学专家的我们也知道，词语之间有明显的所谓“共现”关系，在不同主题的文章中，可能共现的次数或频率有变化，但彼此间绝对谈不上独立。

其二，使用某个词在某个类别训练文档中出现的次数来估计P(wi|Ci)时，只在训练样本数量非常多的情况下才比较准确（考虑扔硬币的问题，得通过大量观察才能基本得出正反面出现的概率都是二分之一的结论，观察次数太少时很可能得到错误的答案），而需要大量样本的要求不仅给前期人工分类的工作带来更高要求（从而成本上升），在后期由计算机处理的时候也对存储和计算资源提出了更高的要求。

**kNN算法**

kNN算法则又有所不同，在kNN算法看来，训练样本就代表了类别的准确信息（因此此算法产生的分类器也叫做“基于实例”的分类器），而不管样本是使用什么特征表示的。其基本思想是在给定新文档后，计算新文档特征向量和训练文档集中各个文档的向量的相似度，得到K篇与该新文档距离最近最相似的文档，根据这K篇文档所属的类别判定新文档所属的类别（注意这也意味着kNN算法根本没有真正意义上的“训练”阶段）。这种判断方法很好的克服了Rocchio算法中无法处理线性不可分问题的缺陷，也很适用于分类标准随时会产生变化的需求（只要删除旧训练文档，添加新训练文档，就改变了分类的准则）。

kNN唯一的也可以说最致命的缺点就是判断一篇新文档的类别时，需要把它与现存的所有训练文档全都比较一遍，这个计算代价并不是每个系统都能够承受的（比如我将要构建的一个文本分类系统，上万个类，每个类即便只有20个训练样本，为了判断一个新文档的类别，也要做20万次的向量比较！）。一些基于kNN的改良方法比如Generalized Instance Set就在试图解决这个问题。

**SVM算法**

支持向量机(Support Vector Machine)是Cortes和Vapnik于1995年首先提出的，它在解决小样本、非线性及高维模式识别中表现出许多特有的优势，并能够推广应用到函数拟合等其他机器学习问题中[10]。

支持向量机方法是建立在统计学习理论的VC 维理论和结构风险最小原理基础上的，根据有限的样本信息在模型的复杂性（即对特定训练样本的学习精度，Accuracy）和学习能力（即无错误地识别任意样本的能力）之间寻求最佳折衷，以期获得最好的推广能力[14]（或称泛化能力）。

SVM 方法有很坚实的理论基础，SVM 训练的本质是解决一个二次规划问题（Quadruple Programming，指目标函数为二次函数，约束条件为线性约束的最优化问题），得到的是全局最优解，这使它有着其他统计学习技术难以比拟的优越性。SVM 分类器的文本分类效果很好，是最好的分类器之一。同时使用核函数将原始的样本空间向高维空间进行变换，能够解决原始样本线性不可分的问题。其缺点是核函数的选择缺乏指导，难以针对具体问题选择最佳的核函数；另外SVM 训练速度极大地受到训练集规模的影响，计算开销比较大，针对SVM 的训练速度问题，研究者提出了很多改进方法，包括Chunking 方法、Osuna 算法、SMO 算法和交互SVM 等等[14]。

SVM分类器的优点在于通用性较好，且分类精度高、分类速度快、分类速度与训练样本个数无关，在查准和查全率方面都优于kNN及朴素贝叶斯方法[8]。

与其它算法相比，SVM算法的理论基础较为复杂，但应用前景很广，我打算专门写一个系列的文章，详细的讨论SVM算法，stay tuned！

介绍过了几个很具代表性的算法之后，不妨用国内外的几组实验数据来比较一下他们的优劣。

在中文语料上的试验，文献[6]使用了复旦大学自然语言处理实验室提供的基准语料对当前的基于词向量空间文本模型的几种分类算法进行了测试，这一基准语料分为20个类别，共有9804篇训练文档，以及9833篇测试文档。在经过统一的分词处理、噪声词消除等预处理之后，各个分类方法的性能指标如下。

**文本分类入门(六)训练Part 3**

其中F1 测度是一种综合了查准率与召回率的指标，只有当两个值均比较大的时候，对应的F1测度才比较大，因此是比单一的查准或召回率更加具有代表性的指标。

由比较结果不难看出，SVM和kNN明显优于朴素贝叶斯方法（但他们也都优于Rocchio方法，这种方法已经很少再参加评测了）。
(六)相关概念总结

- **学习方法**：使用样例（或称样本，训练集）来合成计算机程序的过程称为学习方法[22]。

- **监督学习**：学习过程中使用的样例是由输入/输出对给出时，称为监督学习[22]。最典型的监督学习例子就是文本分类问题，训练集是一些已经明确分好了类别文档组成，文档就是输入，对应的类别就是输出。

- **非监督学习**：学习过程中使用的样例不包含输入/输出对，学习的任务是理解数据产生的过程 [22]。典型的非监督学习例子是聚类，类别的数量，名称，事先全都没有确定，由计算机自己观察样例来总结得出。

- **TSR（Term Space Reduction）**：特征空间的压缩，即降维，也可以叫做特征提取。包括特征选择和特征抽取两大类方法。

- **分类状态得分（CSV，Categorization Status Value)**：用于描述将文档归于某个类别下有多大的可信度。

- **准确率（Precision）**：在所有被判断为正确的文档中，有多大比例是确实正确的。

- **召回率（Recall）**：在所有确实正确的文档中，有多大比例被我们判为正确。

- **假设**：计算机对训练集背后的真实模型（真实的分类规则）的猜测称为假设。可以把真实的分类规则想像为一个目标函数，我们的假设则是另一个函数，假设函数在所有的训练数据上都得出与真实函数相同（或足够接近）的结果。

- **泛化性**：一个假设能够正确分类训练集之外数据（即新的，未知的数据）的能力称为该假设的泛化性[22]。

- **一致假设**：一个假设能够对所有训练数据正确分类，则称这个假设是一致的[22]。

- **过拟合**：为了得到一致假设而使假设变得过度复杂称为过拟合[22]。想像某种学习算法产生了一个过拟合的分类器，这个分类器能够百分之百的正确分类样本数据（即再拿样本中的文档来给它，它绝对不会分错），但也就为了能够对样本完全正确的分类，使得它的构造如此精细复杂，规则如此严格，以至于任何与样本数据稍有不同的文档它全都认为不属于这个类别！

- **超平面（Hyper Plane）**：n维空间中的线性函数唯一确定了一个超平面。一些较直观的例子，在二维空间中，一条直线就是一个超平面；在三维空间中，一个平面就是一个超平面。

- **线性可分和不可分**：如果存在一个超平面能够正确分类训练数据，并且这个程序保证收敛，这种情况称为线形可分。如果这样的超平面不存在，则称数据是线性不可分的[22]。

- **正样本和负样本**：对某个类别来说，属于这个类别的样本文档称为正样本；不属于这个类别的文档称为负样本。

- **规划**：对于目标函数，等式或不等式约束都是线性函数的问题称为线性规划问题。对于目标函数是二次的，而约束都是线性函数的最优化问题称为二次规划问题[22]。

对偶问题：

给定一个带约束的优化问题

目标函数：min f(x)

约束条件：C(x) ≥0

可以通过拉格朗日乘子构造拉格朗日函数

L(x,λ)=f(x)- λTC(x)

令g(λ)= f(x)- λTC(x)

则原问题可以转化为

目标函数：max g(λ)

约束条件：λ≥0

这个新的优化问题就称为原问题的对偶问题（两个问题在取得最优解时达到的条件相同）。  

**(七)中英文文本分类的异同**

**从文本分类系统的处理流程来看，无论待分类的文本是中文还是英文，在训练阶段之前都要经过一个预处理的步骤，去除无用的信息，减少后续步骤的复杂度和计算负担。**

对中文文本来说，首先要经历一个分词的过程，就是把连续的文字流切分成一个一个单独的词汇（因为词汇将作为训练阶段“特征”的最基本单位），例如原文是“中华人民共和国今天成立了”的文本就要被切分成“中华／人民／共和国／今天／成立／了”这样的形式。而对英文来说，没有这个步骤（更严格的说，并不是没有这个步骤，而是英文只需要通过空格和标点便很容易将一个一个独立的词从原文中区分出来）。**中文分词的效果对文本分类系统的表现影响很大**，因为在后面的流程中，全都使用预处理之后的文本信息，不再参考原始文本，因此分词的效果不好，等同于引入了错误的训练数据。分词本身也是一个值得大书特书的问题，**目前比较常用的方法有词典法、隐马尔科夫模型和新兴的CRF方法。**

　　预处理中在分词之后的“去停止词”一步对两者来说是相同的，都是要把语言中一些表意能力很差的辅助性文字从原始文本中去除，对中文文本来说，类似“我们”，“在”，“了”，“的”这样的词汇都会被去除，英文中的“ an”，“in”，“the”等也一样。**这一步骤会参照一个被称为“停止词表”的数据（里面记录了应该被去除的词，有可能是以文件形式存储在硬盘上，也有可能是以数据结构形式放在内存中）来进行。**

　　对中文文本来说，到此就已初审合格，可以参加训练了。而英文文本还有进一步简化和压缩的空间。我们都知道，**英文中同一个词有所谓词形的变化（相对的，词义本身却并没有变），例如名词有单复数的变化，动词有时态的变化，形容词有比较级的变化等等**，还包括这些变化形式的某种组合。而正因为词义本身没有变化，仅仅词形不同的词就不应该作为独立的词来存储和和参与分类计算。去除这些词形不同，**但词义相同的词，仅保留一个副本的步骤就称为“词根还原”，例如在一篇英文文档中，经过词根还原后，“computer”，“compute”，“computing”，“computational”这些词全都被处理成“compute”**（大小写转换也在这一步完成，当然，还要记下这些词的数目作为compute的词频信息）。

经过预处理步骤之后，原始文档转换成了非常节省资源，也便于计算的形式，后面的训练阶段大同小异。
 
**（九）特征选择算法之开方检验**

前文提到过，除了分类算法以外，为分类文本作处理的特征提取算法也对最终效果有巨大影响，而特征提取算法又分为特征选择和特征抽取两大类，其中特征选择算法有互信息，文档频率，信息增益，开方检验等等十数种，这次先介绍特征选择算法中效果比较好的开方检验方法。

大家应该还记得，开方检验其实是数理统计中一种常用的检验两个变量独立性的方法。

开方检验最基本的思想就是通过观察实际值与理论值的偏差来确定理论的正确与否。具体做的时候常常先假设两个变量确实是独立的（行话就叫做“原假设”），然后观察实际值（也可以叫做观察值）与理论值（这个理论值是指“如果两者确实独立”的情况下应该有的值）的偏差程度，如果偏差足够小，我们就认为误差是很自然的样本误差，是测量手段不够精确导致或者偶然发生的，两者确确实实是独立的，此时就接受原假设；如果偏差大到一定程度，使得这样的误差不太可能是偶然产生或者测量不精确所致，我们就认为两者实际上是相关的，即否定原假设，而接受备择假设。

那么用什么来衡量偏差程度呢？假设理论值为E（这也是数学期望的符号哦），实际值为x，如果仅仅使用所有样本的观察值与理论值的差值x-E之和

**（十）特征选择方法之信息增益**

前文提到过，除了开方检验（CHI）以外，信息增益（IG，Information Gain）也是很有效的特征选择方法。但凡是特征选择，总是在将特征的重要程度量化之后再进行选择，而如何量化特征的重要性，就成了各种方法间最大的不同。开方检验中使用特征与类别间的关联性来进行这个量化，关联性越强，特征得分越高，该特征越应该被保留。

　　在信息增益中，重要性的衡量标准就是看特征能够为分类系统带来多少信息，带来的信息越多，该特征越重要。

　　才因此先回忆一下信息论中有关信息量（就是“熵”）的定义。说有这么一个变量X，它可能的取值有n多种，分别是x1，x2，……，xn，每一种取到的概率分别是P1，P2，……，Pn，那么X的熵就定义为：
