## 文本分类,数据挖掘和机器学习

机器学习的有概率分类器(probabilistic) ,贝叶斯推理网络(bayesian inference networks) , 决策树分类器(decision tree) ,决策规则分类器(decision rule) ,基于回归的线性最小二乘llsf(regression based on linearleast squares fit ) , 符号规则归纳法( symbolic rule induction) ,中心向量法(rocchio) ,神经网络法(neural networks) ,k 近邻法(knn) ,支持向量机法(svm) ,投票委员会(majority voting ) , 遗传算法( genetic algorithm) , 最大熵算法(maximum entropy) , ecoc(error correcting output coding) ,等等。这些分类算法成为目前文本分类的主流,在不同的领域里取得了较好的效果。

究竟哪一种算法性能好些至今没有一个定论。实验表明knn ,svm 和贝叶斯分类器的性能比较好。

**(一)文本分类问题的定义**

一个文本（以下基本不区分“文本”和“文档”两个词的含义）**分类问题就是将一篇文档归入预先定义的几个类别中的一个或几个，而文本的自动分类则是使用计算机程序来实现这样的分类。**

注意这个定义当中着重强调的两个事实。

第一，用于分类所需要的类别体系是预先确定的。例如新浪新闻的分类体系，Yahoo!网页导航的分类层次。这种分类层次一旦确定，在相当长的时间内都是不可变的，或者即使要变更，也要付出相当大的代价（基本不亚于推倒并重建一个分类系统）。

第二，一篇文档并没有严格规定只能被分配给一个类别。这与分类这个问题的主观性有关，例如找10个人判断一篇文章所陈述的主题究竟属于金融，银行还是财政政策领域，10个人可能会给出10个不同的答案，因此一篇文章很可能被分配到多个类别当中，只不过分给某些类别让人信服，而有些让人感觉模棱两可罢了（置信度不一样）。

当然，目前真正大量使用文本分类技术的，仍是依据文章主题的分类，而据此构建最多的系统，当属搜索引擎。内里的原因当然不言自明，我只是想给大家提个醒，文本分类还不完全等同于网页分类。网页所包含的信息远比含于其中的文字（文本）信息多得多，对一个网页的分类，除了考虑文本内容的分类以外，链入链出的链接信息，页面文件本身的元数据，甚至是包含此网页的网站结构和主题，都能给分类提供莫大的帮助（比如新浪体育专栏里的网页毫无疑问都是关于体育的），因此说文本分类实际上是网页分类的一个子集也毫不为过。当然，纯粹的文本分类系统与网页分类也不是一点区别都没有。文本分类有个重要前提：即只能根据文章的文字内容进行分类，而不应借助诸如文件的编码格式，文章作者，发布日期等信息。而这些信息对网页来说常常是可用的，有时起到的作用还很巨大！因此纯粹的文本分类系统要想达到相当的分类效果，必须在本身的理论基础和技术含量上下功夫。

除了搜索引擎，诸如数字图书馆，档案管理等等要和海量文字信息打交道的系统，都用得上文本分类。
