### 细说中文分词

完整的中文自然语言处理过程一般包括以下五种中文处理核心技术：分词、词性标注、命名实体识别、依存句法分析、语义分析。其中，分词是中文自然语言处理的基础，搜素引擎、文本挖掘、机器翻译、关键词提取、自动摘要生成等等技术都会用到中文分词，包括最近在学习的聊天机器人、文本相似性等。可以说分词是自然语言大厦的地基，下面就让我们从它开始谈起。

#### 什么是中文分词
中文分词就是将中文语句中的词汇按照使用时的含义切分出来的过程，也就是将一个汉字序列切分成一个个有单独含义的词语。自20世纪80年代以来，中文自动分词就一直是一个研究热点，由于中文语言的复杂性使之一直处于发展阶段。目前，分词主要包含细粒度分词和粗粒度分词两种，在不同的应用场景需要用到不同的粒度。细粒度分词是指将原始语句切分成最基本的词语，而粗粒度分词是指将原始语句中的多个基本词组合起来切成一个词，进而组成语义相对明确的实体。

- 原始串：浙江大学坐落在西湖旁边
- 细粒度：浙江/大学/坐落/在/西湖/旁边
- 粗粒度：浙江大学/坐落/在/西湖/旁边

#### 为什么要中文分词
对于中文而言，词是承载语义的最小单元，由词构成语句，又由语句构成篇章。但是，中文文本是由连续的字序列构成，词与词之间是没有天然的分隔符。在自然语言处理领域，国外已经做出了很多卓有成效的研究，但是那些研究大多基于英文（存在天然的分隔符），也就是说是以正确切分出单词为前提的。于是，NLP对于中文而言要想取得较好的科研成果，就需要准确识别词与词之间的边界，也就是分词。

接下来我们就以搜索为例，具体的阐述一下分词的重要性与必要性。大家都知道，目前的搜素引擎是基于一种叫做倒排索引的结构，以什么作为索引的key值，直接影响到整个搜索引擎的准确度、召回率以及性能。

如果不使用中文分词，可以采用单个汉字索引方式。例如“标点符”，会先索引“标”字，再索引“点”字，再索引“符”字。搜索过程中，也是先寻找“标”字关联的所有文档，再寻找“点”字关联的所有文档，再寻找“符”字关联的所有文档，最后对所有被检索出的文档做“与”运算，同时“标点符”位置连续的文档才算符合要求。这种方式存在一个非常挑战性的问题，常用汉字总共3000左右，每次查询过程中进行“与”操作的计算量会相当大。对于大数据量的搜索引擎来讲，每天面临亿万级别的查询，这样的索引结构无疑是灾难性的。

为了优化上面提到的速度问题，还有另外一种索引结构也是可以避开中文分词的，那就是n元组合索引方式。用2元索引来说，“标点符”，会先索引“标点”，再索引“点符”。在搜索过程中，也是对“标点”和“点符”检索出的文章进行“与”运算。这样的搜索过程会大大减少在搜索过程中的计算量，但是仍会面临另外一个问题：准确度。有很多这样的例子，搜“北大”会检索出“东北大学”，搜“的士”会出现“不想当将军的士兵不是好士兵”。对于大数据量的搜索引擎系统来说，这样的用户体验是极差的。

#### 中文分词面临的挑战
在知道分词的重要性之后，那么我们会面临一个新的问题，如何才能把一个字序列准确的切分成词序列，就像下面的例子会有不止一种的切分方式。

原始字符串：结婚的和尚未结婚的

- 切分一：结婚/的/和尚/未/结婚/的
- 切分二：结婚/的/和/尚未/结婚/的
还有更极端的例子，“中外科学名著”中，“中外”、“外科”、科学”、“学名”、“名著”都是合理的词语。类似的例子数不胜数，“提高产品质量”，“鞭炮声响彻夜空”。在中文分词的世界里，最主要的挑战有两个：歧义词识别，未登录词识别。

#### 歧义词
上文提到的歧义词例子，有学者试图通过逆向匹配来解决。但是，碰到这句“结合成分子”时，采用逆向匹配，则会分成“结合/成分/子时”。一般当一个字可以同时作为两个词的组成部分，当这两个词按序同时出现时，就可能会出现歧义现象。目前的歧义一般分为三种：交叉歧义，组合歧义，真歧义。

交叉歧义（字符串AJB，AJ和JB都是一个汉语词汇，会存在多种切分交叉在一起）：“你说的确实在理”，“的确”和“确实”就是交叉型歧义片段。
组合歧义（字符串AB是一个词汇，A和B同时也是词汇，会存在不同语义下切分不同）：“这个人手上有颗痣”，“目前人手紧缺”。前者是“人”/“手”两个实体词，后者是“人手”一个实体词。
真歧义（怎么切分都合理）：“乒乓球拍卖完了”，切分为以下两种情况都是合理的，“乒乓球拍/卖/完了”，“乒乓球/拍卖/完了”。
#### 未登录词
所谓的未登录词是指在分词词典中没有收录，并且确实是大家公认的词语的那些词语，一般又叫做新词。最典型的未登录词就是人名词，“李胜利喜欢唱歌”中“李胜利”是个人名词，如果把“李胜利”这个基本词条收录到字典中去是能解决这个问题。但是，每时每刻都有新增的姓名，完整收录全部人名本身就是一个不现实的工程。中外人名、中国地名、机构组织名、事件名、货币名、缩略语、派生词、各种专业术语以及在不断发展和约定俗成的一些新词语。在当下的互联网时代，人们还会不断的创造出一些新词出来，比如：“神马”、“不明觉厉”等。未登录词辨别未登录词包括是种类繁多，形态组合各异，规模宏大的一个领域。对这些词语的自动辨识，是一件非常困难的事。

新词是中文分词算法在召回层面上最主要的难题，也是评价一个分词系统好坏的重要标志。如果一个新词无法被分词系统识别，会导致很多噪音数据被召回，进而会影响后面的句法分析和语义分析等相关处理。黄昌宁等在中文信息学报上的《中文分词十年回顾》一文指出：新词带来的分词问题是歧义的10倍~20倍，所以说新词发现是分词面临的最大挑战。

### 中文分词的技术分类
从上世纪80年代开始对中文自动分词进行研究，在过去的近40年中，中文分词的发展过程基本上可分为以下三个阶段，如下图所示：
![p12](https://github.com/yueyuanyang/knowledge/blob/master/ML/img/p12.png)

我们讨论的分词算法可分为三大类：
- 基于词典：基于字典、词库匹配的分词方法；（字符串匹配、机械分词法）
- 基于统计：基于词频度统计的分词方法
- 基于规则：基于知识理解的分词方法

### 基于词典的分词
中文自动分词第一阶段，从80年代到90年代中，以基于词典和人工规则的方法为主，典型的方法有：正向最大匹配，逆向最大匹配，最少词切分法，双向匹配法。这种方法又叫做机械分词方法，它是按照一定的策略将待分析的汉字串与一个“充分大的”机器词典中的词条进行配，若在词典中找到某个字符串，则匹配成功（识别出一个词）。按照扫描方向的不同，串匹配分词方法可以分为正向匹配和逆向匹配；按照不同长度优先匹配的情况，可以分为最大（最长）匹配和最小（最短）匹配；常用的几种机械分词方法如下：

#### 最大正向匹配法(MM, MaximumMatching Method,MM)

通常简称为MM法。其基本思想为：假定分词词典中的最长词有i个汉字字符，则用被处理文档的当前字串中的前i个字作为匹配字段，查找字典。若字典中存在这样的一个i字词，则匹配成功，匹配字段被作为一个词切分出来。如果词典中找不到这样的一个i字词，则匹配失败，将匹配字段中的最后一个字去掉，对剩下的字串重新进行匹配处理…… 如此进行下去，直到匹配成功，即切分出一个词或剩余字串的长度为零为止。这样就完成了一轮匹配，然后取下一个i字字串进行匹配处理，直到文档被扫描完为止。其算法描述如下：

从左向右取待切分汉语句的m个字符作为匹配字段，m为大机器词典中最长词条个数。
查找大机器词典并进行匹配。若匹配成功，则将这个匹配字段作为一个词切分出来。若匹配不成功，则将这个匹配字段的最后一个字去掉，剩下的字符串作为新的匹配字段，进行再次匹配，重复以上过程，直到切分出所有词为止。
举个例子：我们对“南京市长江大桥”这个句子进行分词，根据正向最大匹配的原则。先从句子中拿出前5个字符“南京市长江”，把这5个字符到词典中匹配，发现没有这个词，那就缩短取字个数，取前四个“南京市长”，发现词库有这个词，就把该词切下来；对剩余三个字“江大桥”再次进行正向最大匹配，会切成“江/大桥”；整个句子切分完成为：“京市长/江/大桥。”

#### 逆向最大匹配法(ReverseMaximum Matching Method,RMM)

通常简称为RMM法。RMM法的基本原理与MM法相同 ,不同的是分词切分的方向与MM法相反，而且使用的分词辞典也不同。逆向最大匹配法从被处理文档的末端开始匹配扫描，每次取最末端的2i个字符（i字字串）作为匹配字段，若匹配失败，则去掉匹配字段最前面的一个字，继续匹配。相应地，它使用的分词词典是逆序词典，其中的每个词条都将按逆序方式存放。在实际处理时，先将文档进行倒排处理，生成逆序文档。然后，根据逆序词典，对逆序文档用正向最大匹配法处理即可。

还是那个例子：取出“南京市长江大桥”的后四个字“长江大桥”，发现词典中有匹配，切割下来；对剩余的“南京市”进行分词，整体结果为：“南京市/江大桥”

由于汉语中偏正结构较多，若从后向前匹配，可以适当提高精确度。所以，逆向最大匹配法比正向最大匹配法的误差要小。统计结果表明，单纯使用正向最大匹配的错误率为1/169，单纯使用逆向最大匹配的错误率为1/245。

例如切分字段“硕士研究生产”，正向最大匹配法的结果会是“硕士研究生/产”，而逆向最大匹配法利用逆向扫描，可得到正确的分词结果“硕士/研究/生产”。但这种精度还远远不能满足实际的需要。实际使用的分词系统，都是把机械分词作为一种初分手段，还需通过利用各种其它的语言信息来进一步提高切分的准确率。

#### 双向最大匹配法(Bi-directction Matching method,BM)

双向最大匹配法是将正向最大匹配法得到的分词结果和逆向最大匹配法的到的结果进行比较，从而决定正确的分词方法。据SunM.S. 和 Benjamin K.T.（1995）的研究表明，中文中90.0%左右的句子，正向最大匹配法和逆向最大匹配法完全重合且正确，只有大概9.0%的句子两种切分方法得到的结果不一样，但其中必有一个是正确的（歧义检测成功），只有不到1.0%的句子，或者正向最大匹配法和逆向最大匹配法的切分虽重合却是错的，或者正向最大匹配法和逆向最大匹配法切分不同但两个都不对（歧义检测失败）。这正是双向最大匹配法在实用中文信息处理系统中得以广泛使用的原因所在。

还是那个例子：双向的最大匹配，即把所有可能的最大词都分出来，上面的句子可以分为：南京市、南京市长、长江大桥、江、大桥

#### 最少切分法

使每一句中切出的词数最小。

由于汉语单字成词的特点，正向最小匹配和逆向最小匹配一般很少使用。一般说来，逆向匹配的切分精度略高于正向匹配，遇到的歧义现象也较少。

基于词典分词这类算法优点是速度块，都是O(n)时间复杂度，实现简单，效果尚可。但也有缺点，这种基于规则的机械匹配法缺乏歧义切分处理，上面提到的几种切分方法是从不同的角度来处理歧义问题，但是任何一种方法只能解决有限类别的歧义问题。随着词典的增大，词与词之间的交叉会变得更加严重，歧义带来的负面影响会更加严重。同时，基于规则的切分方法对于新词的切分是完全无能为力的。

#### 基于统计的分词法
中文自动分词第二阶段，从90年代中到03年，分词算法开始引入基于语料库的统计学习方法，最典型的方法就是基于词典全切分加上最大概率路径。首先，介绍一下全切分方法，它是基于词的频度统计的分词方法的基础。全切分顾名思义就是获取原字序列的所有可能成词的切分结果，这样就不会遗漏可能正确的切分方式。所以建立在部分切分基础上的分词方法不管采取何种歧义纠正策略，都可能会遗漏正确的切分，造成分词错误或失败。而建立在全切分基础上的分词方法，由于全切分取得了所有可能的切分形式，因而从根本上避免了可能切分形式的遗漏，克服了部分切分方法的缺陷。

将全切分的结构构件一个有向无环图，比如“杭州亚运会”的全切分有向无环图如下所示。
![p13](https://github.com/yueyuanyang/knowledge/blob/master/ML/img/p13.png)

构成有向无环图之后，在此图中找到一条概率最大的路径，使用的是N元文法模型（N-gram）模型，该模型基于这样一种假设，第n个词的出现只与前面N-1个词相关，而与其它任何词都不相关，整句的概率就是各个词出现概率的乘积。我们给定一个词，然后猜测下一个词是什么。当我说“艳照门”这个词时，你想到下一个词是什么呢？我想大家很有可能会想到“陈冠希”，N-gram模型的主要思想就是这样的。

![p14](https://github.com/yueyuanyang/knowledge/blob/master/ML/img/p14.png)

其中，w值是指用全切分方法切分出来的词语。基于全切分最大概率路径的切分算法也是需要依赖词典，全切分在实际使用过程，一般会通过词典将所有成词的切分方式找出来构成有向无环图。第一阶段的中文分词相比，它也是无法完成识别新词，但是歧义词识别的问题基本被解决。在实际使用的工业分词系统中，词典中的词一般会带有词频属性。同时，还会有一份词与词之间的跳转频率表，最大概率的计算往往是基于词频和词之间的跳转频率进行的。

-全切分算法能取得所有可能的切分形式，它的句子覆盖率和分词覆盖率均为100%，但全切分分词并没有在文本处理中广泛地采用，原因有以下几点：
-全切分算法只是能获得正确分词的前提，因为全切分不具有歧义检测功能，最终分词结果的正确性和完全性依赖于独立的歧义处理方法，如果评测有误，也会造成错误的结果。

全切分的切分结果个数随句子长度的增长呈指数增长，一方面将导致庞大的无用数据充斥于存储数据库；另一方面当句长达到一定长度后，由于切分形式过多,造成分词效率严重下降。

#### 基于规则的分词法

从03年至今，中文分词由基于词的方法开始向基于字的方法转变。当前的方法都是首先根据语料训练分词模型，然后对每一个字进行标注，最后根据标注结果来进行分词。其实就是根据语料训练分类模型，对每一个字进行类别标注，最后根据类别进行分词。这类分词基于人工标注的词性和统计特征，对中文进行建模，即根据观测到的数据（标注好的语料）对模型参数进行估计，即训练。在分词阶段再通过模型计算各种分词出现的概率，将概率最大的分词结果作为最终结果。这类分词算法能很好处理歧义和未登录词问题，效果比前一类效果好，但是需要大量的人工标注数据，以及较慢的分词速度。最典型的方法就是HMM和CRF，其中，CRF比HMM有更弱的上下文无关性假设，当然效果要好一些。

#### 隐马尔科夫模型(Hidden Markov Model, HMM)
全切分这种方法存在两个致命的缺陷：一个缺陷是参数空间过大，不可能实用化；另外一个缺陷是数据稀疏严重。为了解决这个问题，我们引入了马尔科夫假设：一个词的出现仅仅依赖于它前面出现的有限的一个或者几个词。如果一个词的出现仅依赖于它前面出现的一个词，那么我们就称之为bigram。即

P(T) =P(W1W2W3…Wn)=P(W1)P(W2|W1)P(W3|W1W2)…P(Wn|W1W2…Wn-1)≈P(W1)P(W2|W1)P(W3|W2)…P(Wn|Wn-1)

如果一个词的出现仅依赖于它前面出现的两个词，那么我们就称之为trigram。在实践中用的最多的就是bigram和trigram了，而且效果很不错。高于四元的用的很少，因为训练它需要更庞大的语料，而且数据稀疏严重，时间复杂度高，精度却提高的不多。一般的小公司，用到二元的模型就够了，像Google这种巨头，也只是用到了大约四元的程度，它对计算能力和空间的需求都太大了。

设w1,w2,w3,…,wn是长度为n的字符串，规定任意词wi只与它的前两个相关，得到三元概率模型。以此类推，N元模型就是假设当前词的出现概率只同它前面的N-1个词有关。这是一种全切分方法。它不依靠词典,而是将文章中任意两个字同时出现的频率进行统计，次数越高的就可能是一个词。它首先切分出与词表匹配的所有可能的词，运用统计语言模型和决策算法决定最优的切分结果。它的优点在于可以发现所有的切分歧义并且容易将新词提取出来。

![p15](https://github.com/yueyuanyang/knowledge/blob/master/ML/img/p15.png)

HMM模型示意图

#### HMM模型介绍

HMM的典型介绍就是这个模型是一个五元组:

-StatusSet: 状态值集合

-ObservedSet: 观察值集合

-TransProbMatrix: 转移概率矩阵

-EmitProbMatrix: 发射概率矩阵

-InitStatus: 初始状态分布

#### HMM模型可以用来解决三种问题：

- 参数(StatusSet, TransProbMatrix, EmitRobMatrix, InitStatus)已知的情况下，求解观察值序列。(Forward-backward算法)
- 参数(ObservedSet, TransProbMatrix, EmitRobMatrix, InitStatus)已知的情况下，求解状态值序列。(viterbi算法)
- 参数(ObservedSet)已知的情况下，求解(TransProbMatrix, EmitRobMatrix, InitStatus)。(Baum-Welch算法)

其中，第三种问题最玄乎也最不常用，第二种问题最常用，中文分词、语音识别、新词发现、词性标注都有它的一席之地。这里主要介绍第二种问题，即viterbi算法求解状态值序列的方法。




